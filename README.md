<h2 style="text-align: center; font-weight: bold;">How to Build FAIR Domain-Specific Datasets for fine tuning/training NLP models?</h2>

Domain-specifc natural language processing (NLP) models extract data with high accuracy from unstructured text by identifying specialized vocabularies that can be used in different applications. These models are developed by using domain specific data.
Through specialized datasets, researchers fine-tuned pretrained models for protein-protein relationships in the STRING database (1), and accelerated drug development by identifying chemical-gene and drug-drug interactions, as well as predicting peptide toxicity (2-4), and extracting brain connectivity data of neurological disorders (5).
Despite their applications, fields like veterinary medicine and agricultural biology lack NLP-based applications. Barriers include the absence of high-quality domain-specific datasets, small and unbalanced datasets, and insufficient expertise to build datasets (6, 7). Manual annotation, a common necessity in these fields, is time-consuming and prone to bias, affecting model performance. To address this, we propose a training course focused on building FAIR (Findable, Accessible, Interoperable, Reusable) domain-specific datasets (6). 
#### Our target audience are:
    • Researchers looking to adopt NLP solutions for analyzing domain-specific text, even those who lack expertise in AI but have domain knowledge, which is crucial for building annotating data.
    • Computational biology or AI researchers who work on building domain-specific NLP applications who need to overcome dataset scarcity and quality challenges.


